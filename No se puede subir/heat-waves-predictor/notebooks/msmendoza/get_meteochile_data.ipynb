{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-10-29 20:36:06.473\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_daily_temp_history\u001b[0m:\u001b[36m30\u001b[0m - \u001b[1mgetting/updating daily temperature history for station 330020...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "from os import remove\n",
    "from os.path import join\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import pandas as pd\n",
    "from loguru import logger\n",
    "from requests import get\n",
    "\n",
    "\n",
    "def get_daily_temp_history(\n",
    "    input_path: str,\n",
    "    station_id: int,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Gets the daily temperature history for a given station.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_path : str\n",
    "        Path to the directory where the data will be saved.\n",
    "    station_id : str\n",
    "        Chilean national code for the station.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Daily temperature history for the given station.\n",
    "    \"\"\"\n",
    "    logger.info(f\"getting/updating daily temperature history for station {station_id}...\")\n",
    "\n",
    "    # download and uncompress zip file\n",
    "    zip_file_url = f\"https://climatologia.meteochile.gob.cl/application/datos/getDatosSaclim/{station_id}_XXXX_DiarioTs_\"  # noqa: E501\n",
    "    req = get(\n",
    "        zip_file_url,\n",
    "        stream=True,\n",
    "    ).content\n",
    "\n",
    "    zip_file = ZipFile(BytesIO(req))\n",
    "    zip_info = zip_file.infolist()[0]\n",
    "    zip_info.filename = \"daily_temp_history.csv\"\n",
    "\n",
    "    extract_path = f\"{input_path}/stations/{station_id}\"\n",
    "    zip_file.extract(zip_info, path=extract_path)\n",
    "\n",
    "    # # replace ',' with ';' in .csv file to allow correct column separation\n",
    "    # history_path = f\"{input_path}/stations/{station_id}/{zip_info.filename}\"\n",
    "\n",
    "    # with open(history_path, \"r\") as f:\n",
    "    #     lines = f.readlines()\n",
    "    #     lines = map(lambda x: x.replace(\",\", \";\"), lines)\n",
    "\n",
    "    # with open(history_path, \"w\") as f:\n",
    "    #     f.writelines(lines)\n",
    "\n",
    "    # # read and preprocess history data\n",
    "    # h_data = pd.read_csv(history_path, sep=\";\")\n",
    "\n",
    "    # col_names = {\n",
    "    #     \"momento\": \"date\",\n",
    "    #     \"MediaCli_Valor\": \"cond_mean_temp\",\n",
    "    #     \"MediaAri_Valor\": \"mean_temp\",\n",
    "    #     \"NumDatos_Valor\": \"hourly_data_count\",\n",
    "    #     \"Ts00_Valor\": \"00_temp\",\n",
    "    #     \"Ts12_Valor\": \"12_temp\",\n",
    "    #     \"Maxima_Valor\": \"max_temp\",\n",
    "    #     \"FechaMax_Valor\": \"max_temp_date\",\n",
    "    #     \"Minima_Valor\": \"min_temp\",\n",
    "    #     \"FechaMin_Valor\": \"min_temp_date\",\n",
    "    #     \"FechaPro_Valor\": \"process_date\",\n",
    "    # }\n",
    "    # reordered_cols = [\n",
    "    #     \"min_temp\",\n",
    "    #     \"max_temp\",\n",
    "    #     \"mean_temp\",\n",
    "    #     \"cond_mean_temp\",\n",
    "    #     \"hourly_data_count\",\n",
    "    #     \"00_temp\",\n",
    "    #     \"12_temp\",\n",
    "    #     \"min_temp_date\",\n",
    "    #     \"max_temp_date\",\n",
    "    #     \"process_date\",\n",
    "    # ]\n",
    "\n",
    "    # # parse 'date' column\n",
    "    # h_data[\"momento\"] = pd.to_datetime(h_data[\"momento\"], format=\"%d-%m-%Y %H:%M:%S\")\n",
    "\n",
    "    # h_data = (\n",
    "    #     h_data.drop(columns=[\"CodigoNacional\"])\n",
    "    #     .rename(columns=col_names)\n",
    "    #     .set_index(\"date\")\n",
    "    #     .sort_index()[reordered_cols]\n",
    "    # )\n",
    "    # h_data.index = h_data.index.normalize()\n",
    "\n",
    "    # # save history data\n",
    "    # max_date = h_data.index.max().strftime(\"%Y%m%d\")\n",
    "    # file_path = f\"{input_path}/stations/{station_id}/daily_temp_history/{max_date}_update.parquet\"\n",
    "    # h_data.to_parquet(file_path)\n",
    "\n",
    "    # # remove old .csv and .parquet files\n",
    "    # remove(history_path)\n",
    "    # for file in Path(f\"{input_path}/stations/{station_id}/daily_temp_history\").glob(\"*.parquet\"):\n",
    "    #     if file.name != f\"{max_date}_update.parquet\":\n",
    "    #         remove(file)\n",
    "\n",
    "    # logger.info(f\"daily temperature history for station {station_id} successfully saved\")\n",
    "\n",
    "\n",
    "get_daily_temp_history(\"data\", 330020)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
